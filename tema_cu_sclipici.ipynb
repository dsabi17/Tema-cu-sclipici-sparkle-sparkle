{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sktime\n",
    "!pip install pandas\n",
    "!pip install seaborn\n",
    "!pip install sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.datasets import load_from_arff_to_dataframe\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import zipfile\n",
    "import shutil\n",
    "import os\n",
    "import random\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from scipy.stats import skew\n",
    "from scipy.stats import kurtosis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating datasets and spliting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(\"datasets.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sports, y_train_sports = load_from_arff_to_dataframe('RacketSports\\RacketSports_TRAIN.arff')\n",
    "X_test_sports, y_test_sports = load_from_arff_to_dataframe('RacketSports\\RacketSports_TEST.arff')\n",
    "\n",
    "data_mitbih_train = pd.read_csv('ECG\\mitbih_train.csv', header=None)\n",
    "data_mitbih_test = pd.read_csv('ECG\\mitbih_test.csv', header=None)\n",
    "\n",
    "X_train_mitbih = data_mitbih_train.iloc[:, 0:-1]\n",
    "y_train_mitbih = data_mitbih_train.iloc[:, -1]\n",
    "\n",
    "X_test_mitbih = data_mitbih_test.iloc[:, 0:-1]\n",
    "y_test_mitbih = data_mitbih_test.iloc[:, -1]\n",
    "\n",
    "data_ptbdb_abnormal = pd.read_csv('ECG\\ptbdb_abnormal.csv', header=None)\n",
    "data_ptbdb_normal = pd.read_csv('ECG\\ptbdb_normal.csv', header=None)\n",
    "\n",
    "data_ptbdb = pd.concat([data_ptbdb_abnormal, data_ptbdb_normal])\n",
    "\n",
    "X_ptbdb = data_ptbdb.iloc[:, 0:-1]\n",
    "y_ptbdb = data_ptbdb.iloc[:, -1]\n",
    "\n",
    "X_train_ptbdb, X_test_ptbdb, y_train_ptbdb, y_test_ptbdb = train_test_split(X_ptbdb, y_ptbdb, test_size=0.2, random_state=80085)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_sports = ['Badminton_Smash',\n",
    "                'Badminton_Clear',\n",
    "                'Squash_ForehandBoast',\n",
    "                'Squash_BackhandBoast']\n",
    "\n",
    "labels_mitbih = list(range(5))\n",
    "\n",
    "labels_ptbdb = [0, 1]\n",
    "\n",
    "datasets = [y_train_sports, y_test_sports, y_train_mitbih, y_test_mitbih, y_train_ptbdb, y_test_ptbdb]\n",
    "\n",
    "plot_labels = [labels_sports, labels_mitbih, labels_ptbdb]\n",
    "plot_titles = ['RacketSports', 'ECG Heartbeat Categorization Dataset - MIT-BIH', 'ECG Heartbeat Categorization Dataset - PTB Diagnostic']\n",
    "plot_subtitles = ['Train Set', 'Test Set']\n",
    "\n",
    "sns.set(rc={'figure.autolayout': True})\n",
    "\n",
    "bg_color = '#E5E6F0'\n",
    "color_line = '#0F101A'\n",
    "colors = ['#5465FF', '#788BFF', '#9BB1FF', '#BFD7FF', '#D1EAFF', '#E2FDFF']\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", palette=sns.color_palette(colors))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1.1**\n",
    "\n",
    "Grafice ale frecvenței de apariție a fiecărei etichete (clase) în setul de date de antrenare / test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = '3.1/1/'\n",
    "\n",
    "if not os.path.isdir(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "\n",
    "for index in range(len(datasets)):\n",
    "    # bar plot for class imbalance and cute pie on the site\n",
    "    # print(datasets[index].shape)\n",
    "    # print(plot_titles[index // 2])\n",
    "    # print(plot_subtitles[index % 2])\n",
    "    # plt.figure(facecolor=bg_color)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 7), facecolor=bg_color)\n",
    "    fig.suptitle(plot_subtitles[index % 2] + ' - ' + plot_titles[index // 2], fontsize=20)\n",
    "\n",
    "    sns.countplot(ax=axes[0], x=datasets[index], order=np.unique(plot_labels[index // 2]), edgecolor=color_line)\n",
    "    axes[0].set_facecolor(bg_color)\n",
    "\n",
    "    for p in axes[0].patches:\n",
    "        axes[0].bar_label(container=axes[0].containers[0])\n",
    "\n",
    "    counts = [len(datasets[index][datasets[index] == label]) for label in plot_labels[index//2]]\n",
    "    axes[1].pie(counts, labels=plot_labels[index // 2], autopct='%1.1f%%')\n",
    "\n",
    "    plt.savefig(results_dir + plot_titles[index // 2] + '_' + plot_subtitles[index % 2] + '.png')\n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1.2**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Câte un exemplu de serie pentru fiecare tip de acțiune din RacketSports - val de acc accelerometru si de giroscop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(80085)\n",
    "results_dir = '3.1/2/1/'\n",
    "\n",
    "if not os.path.isdir(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "\n",
    "for label in labels_sports:\n",
    "    indexes = random.sample([i for i in range(y_train_sports.shape[0]) if y_train_sports[i] == label], 6)\n",
    "\n",
    "    fig = plt.figure(figsize=plt.figaspect(0.33), facecolor=bg_color)\n",
    "    fig.suptitle('RacketSports ' + label + ' Exemplu - Accelerometru', fontsize=20)\n",
    "    for i in range(6):\n",
    "\n",
    "        x_acc = X_train_sports.to_numpy()[indexes[i]][0].to_numpy()\n",
    "        y_acc = X_train_sports.to_numpy()[indexes[i]][1].to_numpy()\n",
    "        z_acc = X_train_sports.to_numpy()[indexes[i]][2].to_numpy()\n",
    "\n",
    "        ax = fig.add_subplot(2, 3, i + 1, projection='3d', facecolor=bg_color)\n",
    "        ax.plot(x_acc, y_acc, z_acc, color=colors[0], zorder=-1)\n",
    "        ax.plot(x_acc, y_acc, z_acc, color=colors[4], marker='*', markeredgecolor=colors[0], markeredgewidth=1, markersize=10, linestyle=' ')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(results_dir + 'RacketSports_' + label + '_Accelerometru' + '.png')\n",
    "    plt.show()\n",
    "\n",
    "    fig = plt.figure(figsize=plt.figaspect(0.33), facecolor=bg_color)\n",
    "    fig.suptitle('RacketSports ' + label + ' Exemplu - Giroscop', fontsize=20)\n",
    "    for i in range(6):\n",
    "\n",
    "        x_giro = X_train_sports.to_numpy()[indexes[i]][3].to_numpy()\n",
    "        y_giro = X_train_sports.to_numpy()[indexes[i]][4].to_numpy()\n",
    "        z_giro = X_train_sports.to_numpy()[indexes[i]][5].to_numpy()\n",
    "\n",
    "        ax = fig.add_subplot(2, 3, i + 1, projection='3d', facecolor=bg_color)\n",
    "        ax.plot(x_giro, y_giro, z_giro, color=colors[0], zorder=-1)\n",
    "        ax.plot(x_giro, y_giro, z_giro, color=colors[4], marker='*', markeredgecolor=colors[0], markeredgewidth=1, markersize=10, linestyle=' ')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(results_dir + 'RacketSports_' + label + '_Giroscop' + '.png')\n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Afișați câte un exemplu de serie pentru fiecare categorie de aritmie din seturile de date MIT-BIH / PTB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(80085)\n",
    "results_dir = '3.1/2/2/'\n",
    "\n",
    "if not os.path.isdir(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "\n",
    "# 'ECG Heartbeat Categorization Dataset - MIT-BIH', 'ECG Heartbeat Categorization Dataset - PTB Diagnostic'\n",
    "time_sample = list(range(187))\n",
    "\n",
    "for label in labels_mitbih:\n",
    "    indexes = random.sample([i for i in range(y_train_mitbih.shape[0]) if y_train_mitbih[i] == label], 6)\n",
    "\n",
    "    fig = plt.figure(figsize=plt.figaspect(0.33), facecolor=bg_color)\n",
    "    fig.suptitle('ECG Heartbeat Categorization Dataset - MIT-BIH ' + str(label) , fontsize=20)\n",
    "    for i in range(6):\n",
    "        series = X_train_mitbih.to_numpy()[indexes[i]]\n",
    "\n",
    "        ax = fig.add_subplot(2, 3, i + 1, facecolor=bg_color)\n",
    "        ax.plot(time_sample, series, color=colors[0], zorder=-1)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(results_dir + 'ECG_Heartbeat_Categorization_Dataset_MIT_BIH_' + str(label) + '.png')\n",
    "    plt.show()\n",
    "\n",
    "for label in labels_ptbdb:\n",
    "    indexes = random.sample([i for i in range(y_train_ptbdb.shape[0]) if y_train_ptbdb.to_numpy()[i] == label], 6)\n",
    "\n",
    "    fig = plt.figure(figsize=plt.figaspect(0.33), facecolor=bg_color)\n",
    "    fig.suptitle('ECG Heartbeat Categorization Dataset - PTB Diagnostic ' + str(label) , fontsize=20)\n",
    "    for i in range(6):\n",
    "        series = X_train_ptbdb.to_numpy()[indexes[i]]\n",
    "\n",
    "        ax = fig.add_subplot(2, 3, i + 1, facecolor=bg_color)\n",
    "        ax.plot(time_sample, series, color=colors[0], zorder=-1)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(results_dir + 'ECG_Heartbeat_Categorization_Dataset_PTB_Diagnostic_' + str(label) + '.png')\n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Pentru seturile de date cu aritmii afișați un grafic al mediei și deviației standard\n",
    "per unitate de timp, pentru fiecare clasă de aritmie. Media și deviația standard se\n",
    "calculează peste toate exemplele (atât din train, cât și din train set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = '3.1/2/3/'\n",
    "\n",
    "if not os.path.isdir(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "\n",
    "X_mitbih = pd.concat([X_train_mitbih, X_test_mitbih]).to_numpy()\n",
    "y_mitbih = pd.concat([y_train_mitbih, y_test_mitbih]).to_numpy()\n",
    "\n",
    "\n",
    "for label in labels_mitbih:\n",
    "    indexes = [i for i in range(y_mitbih.shape[0]) if y_mitbih[i] == label]\n",
    "\n",
    "    std_mitbih = np.std(X_mitbih[indexes], axis=0)\n",
    "    mean_mitbih = np.mean(X_mitbih[indexes], axis=0)\n",
    "\n",
    "    fig = plt.figure(facecolor=bg_color)\n",
    "    fig.suptitle('ECG Heartbeat Categorization Dataset - MIT-BIH ' + str(label) , fontsize=20)\n",
    "    ax = fig.add_subplot(1, 1, 1, facecolor=bg_color)\n",
    "    ax.set_title('Standard Deviation and Mean')\n",
    "    ax.plot(time_sample, std_mitbih, color=colors[3], linewidth=2.5)\n",
    "    ax.plot(time_sample, mean_mitbih, color=colors[2], linewidth=2, zorder=-1)\n",
    "\n",
    "    plt.savefig(results_dir + 'ECG_Heartbeat_Categorization_Dataset_MIT_BIH_mean_std_' + str(label) + '.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for label in labels_ptbdb:\n",
    "    indexes = [i for i in range(y_ptbdb.shape[0]) if y_ptbdb.to_numpy()[i] == label]\n",
    "    std_ptbdb = np.std(X_ptbdb.to_numpy()[indexes], axis=0)\n",
    "    mean_ptbdb = np.mean(X_ptbdb.to_numpy()[indexes], axis=0)\n",
    "\n",
    "    fig = plt.figure(facecolor=bg_color)\n",
    "    fig.suptitle('ECG Heartbeat Categorization Dataset - PTB Diagnostic ' + str(label) , fontsize=20)\n",
    "    ax = fig.add_subplot(1, 1, 1, facecolor=bg_color)\n",
    "    ax.set_title('Standard Deviation and Mean')\n",
    "    ax.plot(time_sample, std_ptbdb, color=colors[3], linewidth=2.5)\n",
    "    ax.plot(time_sample, mean_ptbdb, color=colors[2], linewidth=2, zorder=-1)\n",
    "    plt.savefig(results_dir + 'ECG_Heartbeat_Categorization_Dataset_PTB_Diagnostic_mean_std_' + str(label) + '.png')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Pentru setul de date RacketSports afișați distribuția valorilor per fiecare axă de\n",
    "accelerometru și giroscop în parte / per acțiune (a se vedea un exemplu fictiv de\n",
    "output în Figura 1). O astfel de analiză este utilă pentru a determina dacă există niște\n",
    "șabloane imediate care se pot observa în termen de valori specifice pe x, y și z\n",
    "pentru fiecare gest în parte. Aceste șabloane (e.g. intervale de valori specifice) pot fi\n",
    "folosite în etapa de definire a atributelor și pot totodată informa dacă problema este\n",
    "una ușoară (șabloane de valori clar diferențiabile per gest) sau grea (distribuții\n",
    "similare per fiecare gest).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sports = pd.concat([X_train_sports, X_test_sports])\n",
    "data_sports['label'] = np.append(y_train_sports, y_test_sports)\n",
    "\n",
    "columns_sports = data_sports.columns\n",
    "print(columns_sports)\n",
    "\n",
    "# for label in labels_sports:\n",
    "#     indexes = [i for i in range(y_train_sports.shape[0]) if y_train_sports[i] == label]\n",
    "#     data_sports_parsed = pd.DataFrame()\n",
    "#     working_data = X_train_sports.iloc[indexes]\n",
    "#     # print(working_data.columns)\n",
    "#     for col in columns_sports:\n",
    "#         values = np.zeros((30 * len(working_data)))\n",
    "#         for i in range(30 * len(working_data)):\n",
    "#             values[i] = working_data['dim_0'].to_numpy()[i//30][i % 30]\n",
    "#         data_sports_parsed[col] = values\n",
    "\n",
    "#     for col in columns_sports:\n",
    "#         sns.histplot(data=data_sports_parsed, x=col, color=)\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# print(values)\n",
    "# sns.histplot(data=X_train_sports, kde=True)\n",
    "# Unde <nume variabila target> reprezinta numele atributului din setul de date ce identifică\n",
    "# eticheta gestului, <numar clase posibile> este 4 pentru setul de date RacketSport, iar <nume\n",
    "# dimensiune x, y, sau z> este numele atributului pe care îl dați la încărcare setului de 300\n",
    "# valori care corespund axei x, y sau z după caz.\n",
    "\n",
    "# print(X_train_sports)\n",
    "# print(y_train_sports.type)\n",
    "\n",
    "data_sports_parsed = pd.DataFrame()\n",
    "for col in columns_sports:\n",
    "    values = np.zeros((30 * len(data_sports)))\n",
    "    if col != 'label':\n",
    "        for i in range(30 * len(data_sports)):\n",
    "            values[i] = data_sports[col].to_numpy()[i//30][i % 30]\n",
    "        data_sports_parsed[col] = values\n",
    "    else:\n",
    "        values = np.empty((30 * len(data_sports)), dtype='object')\n",
    "        for i in range(30 * len(data_sports)):\n",
    "            values[i] = data_sports[col].to_numpy()[i//30]\n",
    "        print(values)\n",
    "\n",
    "for col in columns_sports:\n",
    "    if col != 'label':\n",
    "        sns.histplot(data=data_sports_parsed, x=col)\n",
    "plt.show()\n",
    "\n",
    "# sns.displot(data_sports_parsed, x='dim_0', hue='label', binwidth=3, height=3, facet_kws=dict(margin_titles=True))\n",
    "# g = sns.FacetGrid(data_sports_parsed, col='label', height=3.5, aspect=.65)\n",
    "# g.map(sns.histplot, \"dim_0\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2.1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pentru analiza celor două seturi de date veți folosi următorii algoritmi:\n",
    "\n",
    "\n",
    "● RandomForest - folosiți implementarea din scikit-learn [0.5p]\n",
    "● GradientBoosted Trees - folosiți implementarea din biblioteca xgboost [0.75p]\n",
    "● SVM - folosiți implementarea din scikit-learn [0.75p]\n",
    "\n",
    "\n",
    "\n",
    "Folosiți înțelegerea datelor câștigată la pasul 3.1 pentru a determina dacă este necesară\n",
    "standardizarea datelor. Acest pas este unul des întâlnit etapă de pre-procesare a datelor\n",
    "înainte de antrenarea unui clasificator, în vederea uniformizării valorilor numerice aferente\n",
    "fiecărui tip de atribut (e.g. nu este dorit ca unele atribute sa aibă valori de ordinul miilor, iar\n",
    "altele de ordinul unităților).\n",
    "\n",
    "\n",
    "\n",
    "Partea de extragere a atributelor propusă în secțiunea 3.2.2 poate duce la un număr mare\n",
    "de atribute extrase. Frecvent se întâmplă ca nu toate atributele să aibă o contribuție\n",
    "importantă în cadrul predicției.\n",
    "Ca atare, investigați aplicarea tehnicilor de selectare a atributelor (eng. Feature selection)\n",
    "oferite în scikit-learn. Folosiți cel puțin una din metodele Variance Threshold sau Select\n",
    "Percentile.\n",
    "\n",
    "\n",
    "Fiecare algoritm din cei propuși are o serie de hiper-parametrii care influențează\n",
    "funcționarea acestuia. Pentru a găsi valorile potrivite pentru aceștia veți folosi o procedură\n",
    "de căutare a hiper-parametrilor pe bază de Grid Search cu Cross Validation.\n",
    "Setul minim de hiper-parametrii de căutat este:\n",
    "\n",
    "\n",
    "● SVM: tipul de kernel, parametru C de regularizare\n",
    "● RandomForest: numărul de arbori, adâncimea maximă a unui arbore, procentul din\n",
    "input folosit la antrenarea fiecărui arbore\n",
    "● GradientBoostedTrees: numărul de arbori, adâncimea maximă a unui arbore,\n",
    "learning rate\n",
    "\n",
    "\n",
    "Evaluarea algoritmilor\n",
    "În raportul vostru trebuie sa prezentați următoarele:\n",
    "● Rezultatul procedurii de feature selection: numărul total de feature-uri considerate și\n",
    "numărul total de feature-uri utilizate la antrenare (ca urmare a procedurii de feature\n",
    "selection)\n",
    "● Pentru fiecare algoritm, realizați un tabel în care să prezentați media si varianța\n",
    "pentru acuratețea generală de clasificare, precizie / recall / F1 la nivelul fiecărei\n",
    "clase în parte\n",
    "○ Pe linii va fi indexată configurația de hiper-parametrii rezultată din procedura\n",
    "de GridSearch.\n",
    "○ Pe coloane vor fi prezentate metricile cerute\n",
    "○ Relevați prin bolduire valorile maxime pentru fiecare metrică\n",
    "● Pentru cea mai bună variantă a hiper-parametrilor, pentru fiecare algoritm,\n",
    "realizați o matrice de confuzie peste clase."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2.2\n",
    "\n",
    "\n",
    "A. Extragere de atribute pentru RacketSports [1p]\n",
    "Datele din RacketSports reprezinta valori continue pe fiecare axă ale unor acțiuni cu 300 de\n",
    "observații per serie. Ca atare, setul de atribute agregate pe care le putem extrage se referă\n",
    "la analiza statistică a semnalelor pe fiecare axă.\n",
    "Sugestii de atribute statistice de extras per serie (sau sub-fereastră a seriei) / per axă:\n",
    "\n",
    "\n",
    "● medie\n",
    "\n",
    "● abaterea standard\n",
    "\n",
    "● abaterea medie absolută\n",
    "\n",
    "● valoare minimă\n",
    "\n",
    "● valoare maximă\n",
    "\n",
    "● diferenta de valori maxime si minime\n",
    "\n",
    "● mediană\n",
    "\n",
    "● abaterea mediană absolută\n",
    "\n",
    "● intervalul intercuartil\n",
    "\n",
    "● Număr de valori negative\n",
    "\n",
    "● Număr de valori pozitive\n",
    "\n",
    "● număr de valori peste medie\n",
    "\n",
    "● număr de vârfuri\n",
    "\n",
    "● Energia semnalului\n",
    "\n",
    "○ Energia unui semnal pe fiecare axă este calculată luând media sumei\n",
    "\n",
    "pătratelor valorilor dintr-o fereastră pe axa respectivă.\n",
    "\n",
    "● Asimetrie (skewness)\n",
    "\n",
    "● Curtoză (kurtosis)\n",
    "\n",
    "● Accelerația medie rezultantă\n",
    "\n",
    "○ Accelerația medie rezultantă peste fereastră este calculată luând media\n",
    "rădăcinilor pătrate ale valorilor din fiecare dintre cele trei axe pătrate și\n",
    "adunate împreună.\n",
    "\n",
    "● Aria mărimii semnalului\n",
    "\n",
    "○ Aria mărimii semnalului este definită ca suma valorilor absolute ale celor\n",
    "trei axe mediate pe o fereastră.\n",
    "\n",
    "\n",
    "\n",
    "În afară de atributele statistice, pentru secvențe numerice interpretate ca semnale se pot\n",
    "calcula atribute extrase pe baza interpretării în regim de frecvență a semnalului (i.e.\n",
    "aplicând o transformată Fourier).\n",
    "Pentru exemple și cod de extragere a atributelor, atât statistice cât și Fourier, urmăriți acest\n",
    "tutorial.\n",
    "B. Extragere de atribute pentru Seturile de date cu Aritmii [1p]\n",
    "Pentru seturile MIT-BIH si PBT veți încerca să antrenați algoritmii de la punctul 3.2.1\n",
    "folosind:\n",
    "● Direct datele de intrare (fiecare din cele 187 de puncte ale unei serii\n",
    "reprezentând bătaia inimii este un atribut)\n",
    "● Atributele statistice descrise la litera A.\n",
    "NOTĂ: extragerile de atribute prezentate mai sus pot fi aplicate pe toată lungimea\n",
    "seriei sau pot fi aplicate pe ferestre de lungime H, unde H < lungimea secvenței.\n",
    "Aceasta înseamnă că puteți împărți secvența voastră în subsecvențe (cu sau făra\n",
    "suprapunere) și să calculați atributele pe fiecare subsecvență în parte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class feature_extractor:\n",
    "    def __init__(self, time_series, labels, W=20, sample_step=15):\n",
    "        self.time_series = time_series\n",
    "        self.labels = labels\n",
    "        self.W = W\n",
    "\n",
    "        # self.data = [(self.time_series[index][i:i + self.W], index)\n",
    "        #              for index in range(len(self.time_series))\n",
    "        #              for i in range(0, len(self.time_series[index]), sample_step)]\n",
    "\n",
    "        # self.indexes = [self.labels[i // len(self.time_series)] + '_' + str(i % len(self.time_series)) for i in range(len(self.data))]\n",
    "\n",
    "        self.data = []\n",
    "        self.indexed_labels = []\n",
    "\n",
    "        for i in range(len(time_series)):\n",
    "            count = 0\n",
    "            for step in range(0, len(self.time_series[i]), sample_step):\n",
    "                self.data.append(self.time_series[i][step : step + self.W])\n",
    "                self.indexed_labels.append(self.labels[i] + '_' + str(count))\n",
    "                count += 1\n",
    "        self.count = count\n",
    "\n",
    "    def get_max(self):\n",
    "        return [np.max(window) for window in self.data], [label + '_max' for label in self.indexed_labels]\n",
    "\n",
    "    def get_min(self):\n",
    "        return [np.min(window) for window in self.data], [label + '_min' for label in self.indexed_labels]\n",
    "\n",
    "    def get_std(self):\n",
    "        return [np.std(window) for window in self.data], [label + '_std' for label in self.indexed_labels]\n",
    "\n",
    "    def get_ptp(self):\n",
    "        return [np.ptp(window) for window in self.data], [label + '_ptp' for label in self.indexed_labels]\n",
    "\n",
    "    def get_avg(self):\n",
    "        return [np.average(window) for window in self.data], [label + '_avg' for label in self.indexed_labels]\n",
    "\n",
    "    def get_median(self):\n",
    "        return [np.median(window) for window in self.data], [label + '_median' for label in self.indexed_labels]\n",
    "\n",
    "    def get_mean(self):\n",
    "        return [np.mean(window) for window in self.data], [label + '_mean' for label in self.indexed_labels]\n",
    "\n",
    "    def get_pos(self):\n",
    "        return [len(window[window >= 0]) for window in self.data], [label + '_pos' for label in self.indexed_labels]\n",
    "\n",
    "    def get_neg(self):\n",
    "        return [len(window[window < 0]) for window in self.data], [label + '_neg' for label in self.indexed_labels]\n",
    "\n",
    "    def get_iqr(self):\n",
    "        return [np.percentile(window, 75) - np.percentile(window, 25) for window in self.data], [label + '_iqr' for label in self.indexed_labels]\n",
    "\n",
    "    def get_over_avg(self):\n",
    "        return [len(window[window > np.average(window)]) for window in self.data], [label + '_over_avg' for label in self.indexed_labels]\n",
    "\n",
    "    def get_peaks(self):\n",
    "        return [len(find_peaks(window)[0]) for window in self.data], [label + '_peaks' for label in self.indexed_labels]\n",
    "\n",
    "    def get_skew(self):\n",
    "        return [skew(window) for window in self.data], [label + '_skew' for label in self.indexed_labels]\n",
    "\n",
    "    def get_kurtosis(self):\n",
    "        return [kurtosis(window) for window in self.data], [label + '_kurtosis' for label in self.indexed_labels]\n",
    "\n",
    "    def get_energy(self):\n",
    "        return [np.sum(window ** 2) / self.W for window in self.data], [label + '_energy' for label in self.indexed_labels]\n",
    "\n",
    "    def get_acc(self):\n",
    "        return [np.average(np.sqrt(self.data[start + i] ** 2 +\n",
    "                self.data[start + i + self.count] ** 2 +\n",
    "                self.data[start + i + self.count * 2] ** 2))\n",
    "                for start in range(0, len(self.data) , 3 * self.count)\n",
    "                for i in range(self.count)], 'acc_'\n",
    "\n",
    "    def get_aria(self):\n",
    "        return [np.sum(np.abs(self.data[start + i]) +\n",
    "                np.abs(self.data[start + i + self.count]) +\n",
    "                np.abs(self.data[start + i + self.count * 2]))\n",
    "                for start in range(0, len(self.data) , 3 * self.count)\n",
    "                for i in range(self.count)], 'aria_'\n",
    "\n",
    "    def get_all(self, no_3D = False):\n",
    "        results_max, labels_max = self.get_max()\n",
    "        results_min, labels_min = self.get_min()\n",
    "        results_std, labels_std = self.get_std()\n",
    "        results_ptp, labels_ptp = self.get_ptp()\n",
    "        results_avg, labels_avg = self.get_avg()\n",
    "        results_median, labels_median = self.get_median()\n",
    "        results_mean, labels_mean = self.get_mean()\n",
    "        results_pos, labels_pos = self.get_pos()\n",
    "        results_neg, labels_neg = self.get_neg()\n",
    "        results_iqr, labels_iqr = self.get_iqr()\n",
    "        results_over_avg, labels_over_avg = self.get_over_avg()\n",
    "        results_peaks, labels_peaks = self.get_peaks()\n",
    "        results_skew, labels_skew = self.get_skew()\n",
    "        results_kurtosis, labels_kurtosis = self.get_kurtosis()\n",
    "        results_energy, labels_energy = self.get_energy()\n",
    "\n",
    "        results = [*results_max, *results_min, *results_std, *results_ptp, *results_avg, *results_median, *results_mean,\n",
    "                   *results_pos, *results_neg, *results_iqr, *results_over_avg, *results_peaks, *results_skew,\n",
    "                   *results_kurtosis, *results_energy]\n",
    "        final_labels = [*labels_max, *labels_min, *labels_std, *labels_ptp, *labels_avg, *labels_median, *labels_mean,\n",
    "                        *labels_pos, *labels_neg, *labels_iqr, *labels_over_avg, *labels_peaks, *labels_skew,\n",
    "                        *labels_kurtosis, *labels_energy]\n",
    "\n",
    "        if no_3D == False:\n",
    "            results_acc, label_acc = self.get_acc()\n",
    "            results_aria, label_aria = self.get_aria()\n",
    "\n",
    "            labels_acc = [label_acc + str(i) for i in range(len(results_acc))]\n",
    "            labels_aria = [label_aria + str(i) for i in range(len(results_aria))]\n",
    "\n",
    "            results = [*results, *results_acc, *results_aria]\n",
    "            final_labels = [*final_labels, *labels_acc, *labels_aria]\n",
    "\n",
    "        return {final_labels[i]:[results[i]] for i in range(len(results))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RacketSports Test\n",
    "time_series_x_acc = X_train_sports['dim_0'].to_numpy()[0].to_numpy()\n",
    "time_series_y_acc = X_train_sports['dim_1'].to_numpy()[0].to_numpy()\n",
    "time_series_z_acc = X_train_sports['dim_2'].to_numpy()[0].to_numpy()\n",
    "time_series_x_giro = X_train_sports['dim_3'].to_numpy()[0].to_numpy()\n",
    "time_series_y_giro = X_train_sports['dim_4'].to_numpy()[0].to_numpy()\n",
    "time_series_z_giro = X_train_sports['dim_5'].to_numpy()[0].to_numpy()\n",
    "\n",
    "time_series = [time_series_x_acc,  time_series_y_acc, time_series_z_acc, time_series_x_giro,  time_series_y_giro, time_series_z_giro]\n",
    "time_series_fft = np.fft.fft([time_series_x_acc,  time_series_y_acc, time_series_z_acc, time_series_x_giro,  time_series_y_giro, time_series_z_giro])\n",
    "\n",
    "# fe = feature_extractor([*time_series, *time_series_fft], ['dim_0', 'dim_1', 'dim_2', 'dim_3', 'dim_4', 'dim_5', 'dim_0_fft', 'dim_1_fft', 'dim_2_fft', 'dim_3_fft', 'dim_4_fft', 'dim_5_fft'])\n",
    "fe = feature_extractor(time_series, ['dim_0', 'dim_1', 'dim_2', 'dim_3', 'dim_4', 'dim_5'])\n",
    "\n",
    "results = fe.get_all()\n",
    "\n",
    "features_sports_train = pd.DataFrame.from_dict(results)\n",
    "\n",
    "for i in range(1, len(X_train_sports)):\n",
    "    time_series_x_acc = X_train_sports['dim_0'].to_numpy()[i].to_numpy()\n",
    "    time_series_y_acc = X_train_sports['dim_1'].to_numpy()[i].to_numpy()\n",
    "    time_series_z_acc = X_train_sports['dim_2'].to_numpy()[i].to_numpy()\n",
    "    time_series_x_giro = X_train_sports['dim_3'].to_numpy()[i].to_numpy()\n",
    "    time_series_y_giro = X_train_sports['dim_4'].to_numpy()[i].to_numpy()\n",
    "    time_series_z_giro = X_train_sports['dim_5'].to_numpy()[i].to_numpy()\n",
    "\n",
    "    time_series = [time_series_x_acc,  time_series_y_acc, time_series_z_acc, time_series_x_giro,  time_series_y_giro, time_series_z_giro]\n",
    "    time_series_fft = np.fft.fft([time_series_x_acc,  time_series_y_acc, time_series_z_acc, time_series_x_giro,  time_series_y_giro, time_series_z_giro])\n",
    "\n",
    "    # fe = feature_extractor([*time_series, *time_series_fft], ['dim_0', 'dim_1', 'dim_2', 'dim_3', 'dim_4', 'dim_5', 'dim_0_fft', 'dim_1_fft', 'dim_2_fft', 'dim_3_fft', 'dim_4_fft', 'dim_5_fft'])\n",
    "    fe = feature_extractor(time_series, ['dim_0', 'dim_1', 'dim_2', 'dim_3', 'dim_4', 'dim_5'])\n",
    "\n",
    "    results = fe.get_all()\n",
    "\n",
    "    features_sports_train = pd.concat([features_sports_train, pd.DataFrame.from_dict(results)], ignore_index=True)\n",
    "\n",
    "features_sports_train.to_csv('features_sports_train.csv')\n",
    "\n",
    "# RacketSports Train\n",
    "time_series_x_acc = X_test_sports['dim_0'].to_numpy()[0].to_numpy()\n",
    "time_series_y_acc = X_test_sports['dim_1'].to_numpy()[0].to_numpy()\n",
    "time_series_z_acc = X_test_sports['dim_2'].to_numpy()[0].to_numpy()\n",
    "time_series_x_giro = X_test_sports['dim_3'].to_numpy()[0].to_numpy()\n",
    "time_series_y_giro = X_test_sports['dim_4'].to_numpy()[0].to_numpy()\n",
    "time_series_z_giro = X_test_sports['dim_5'].to_numpy()[0].to_numpy()\n",
    "\n",
    "time_series = [time_series_x_acc,  time_series_y_acc, time_series_z_acc, time_series_x_giro,  time_series_y_giro, time_series_z_giro]\n",
    "time_series_fft = np.fft.fft([time_series_x_acc,  time_series_y_acc, time_series_z_acc, time_series_x_giro,  time_series_y_giro, time_series_z_giro])\n",
    "\n",
    "fe = feature_extractor(time_series, ['dim_0', 'dim_1', 'dim_2', 'dim_3', 'dim_4', 'dim_5'])\n",
    "# fe = feature_extractor([*time_series, *time_series_fft], ['dim_0', 'dim_1', 'dim_2', 'dim_3', 'dim_4', 'dim_5', 'dim_0_fft', 'dim_1_fft', 'dim_2_fft', 'dim_3_fft', 'dim_4_fft', 'dim_5_fft'])\n",
    "\n",
    "results = fe.get_all()\n",
    "\n",
    "features_sports_test = pd.DataFrame.from_dict(results)\n",
    "\n",
    "for i in range(1, len(X_test_sports)):\n",
    "    time_series_x_acc = X_test_sports['dim_0'].to_numpy()[i].to_numpy()\n",
    "    time_series_y_acc = X_test_sports['dim_1'].to_numpy()[i].to_numpy()\n",
    "    time_series_z_acc = X_test_sports['dim_2'].to_numpy()[i].to_numpy()\n",
    "    time_series_x_giro = X_test_sports['dim_3'].to_numpy()[i].to_numpy()\n",
    "    time_series_y_giro = X_test_sports['dim_4'].to_numpy()[i].to_numpy()\n",
    "    time_series_z_giro = X_test_sports['dim_5'].to_numpy()[i].to_numpy()\n",
    "\n",
    "    time_series = [time_series_x_acc,  time_series_y_acc, time_series_z_acc, time_series_x_giro,  time_series_y_giro, time_series_z_giro]\n",
    "    time_series_fft = np.fft.fft([time_series_x_acc,  time_series_y_acc, time_series_z_acc, time_series_x_giro,  time_series_y_giro, time_series_z_giro])\n",
    "\n",
    "    fe = feature_extractor(time_series, ['dim_0', 'dim_1', 'dim_2', 'dim_3', 'dim_4', 'dim_5'])\n",
    "    # fe = feature_extractor([*time_series, *time_series_fft], ['dim_0', 'dim_1', 'dim_2', 'dim_3', 'dim_4', 'dim_5', 'dim_0_fft', 'dim_1_fft', 'dim_2_fft', 'dim_3_fft', 'dim_4_fft', 'dim_5_fft'])\n",
    "\n",
    "    results = fe.get_all()\n",
    "\n",
    "    features_sports_test = pd.concat([features_sports_test, pd.DataFrame.from_dict(results)], ignore_index=True)\n",
    "\n",
    "features_sports_test.to_csv('features_sports_test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mitbih train\n",
    "\n",
    "time_series = X_train_mitbih.to_numpy()[0]\n",
    "fe = feature_extractor([time_series], ['series'], W = 100, sample_step=12)\n",
    "results = fe.get_all(no_3D=True)\n",
    "features_mitbih_train = pd.DataFrame.from_dict(results)\n",
    "\n",
    "for i in range(1, len(X_train_mitbih)):\n",
    "    print(i)\n",
    "    time_series = X_train_mitbih.to_numpy()[i]\n",
    "\n",
    "    # time_series_fft = np.fft.fft([time_series_x_acc,  time_series_y_acc, time_series_z_acc, time_series_x_giro,  time_series_y_giro, time_series_z_giro])\n",
    "\n",
    "    fe = feature_extractor([time_series], ['series'], W = 100, sample_step=12)\n",
    "    # fe = feature_extractor([*time_series, *time_series_fft], ['dim_0', 'dim_1', 'dim_2', 'dim_3', 'dim_4', 'dim_5', 'dim_0_fft', 'dim_1_fft', 'dim_2_fft', 'dim_3_fft', 'dim_4_fft', 'dim_5_fft'])\n",
    "\n",
    "    results = fe.get_all(no_3D=True)\n",
    "\n",
    "    features_mitbih_train = pd.concat([features_mitbih_train, pd.DataFrame.from_dict(results)], ignore_index=True)\n",
    "\n",
    "features_mitbih_train.to_csv('features_mitbih_train.csv')\n",
    "\n",
    "# Mitbih test\n",
    "time_series = X_test_mitbih.to_numpy()[0]\n",
    "fe = feature_extractor([time_series], ['series'], W = 100, sample_step=12)\n",
    "results = fe.get_all(no_3D=True)\n",
    "features_mitbih_test = pd.DataFrame.from_dict(results)\n",
    "\n",
    "for i in range(1, len(X_test_mitbih)):\n",
    "    time_series = X_test_mitbih.to_numpy()[i]\n",
    "\n",
    "    # time_series_fft = np.fft.fft([time_series_x_acc,  time_series_y_acc, time_series_z_acc, time_series_x_giro,  time_series_y_giro, time_series_z_giro])\n",
    "\n",
    "    fe = feature_extractor([time_series], ['series'], W = 100, sample_step=12)\n",
    "    # fe = feature_extractor([*time_series, *time_series_fft], ['dim_0', 'dim_1', 'dim_2', 'dim_3', 'dim_4', 'dim_5', 'dim_0_fft', 'dim_1_fft', 'dim_2_fft', 'dim_3_fft', 'dim_4_fft', 'dim_5_fft'])\n",
    "\n",
    "    results = fe.get_all(no_3D=True)\n",
    "\n",
    "    features_mitbih_test = pd.concat([features_mitbih_test, pd.DataFrame.from_dict(results)], ignore_index=True)\n",
    "\n",
    "features_mitbih_test.to_csv('features_mitbih_test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ptbdb train\n",
    "print(len(X_train_ptbdb))\n",
    "for i in range(len(X_train_ptbdb)):\n",
    "    print(i)\n",
    "    time_series = X_train_ptbdb.to_numpy()[i]\n",
    "\n",
    "    # time_series_fft = np.fft.fft([time_series_x_acc,  time_series_y_acc, time_series_z_acc, time_series_x_giro,  time_series_y_giro, time_series_z_giro])\n",
    "\n",
    "    fe = feature_extractor([time_series], ['series'], W = 100, sample_step=12)\n",
    "    # fe = feature_extractor([*time_series, *time_series_fft], ['dim_0', 'dim_1', 'dim_2', 'dim_3', 'dim_4', 'dim_5', 'dim_0_fft', 'dim_1_fft', 'dim_2_fft', 'dim_3_fft', 'dim_4_fft', 'dim_5_fft'])\n",
    "\n",
    "    results = fe.get_all(no_3D=True)\n",
    "\n",
    "    if i == 0:\n",
    "        features_ptbdb_train = pd.DataFrame.from_dict(results)\n",
    "    else:\n",
    "        features_ptbdb_train = pd.concat([features_ptbdb_train, pd.DataFrame.from_dict(results)], ignore_index=True)\n",
    "\n",
    "features_ptbdb_train.to_csv('features_ptbdb_train.csv')\n",
    "\n",
    "# Ptbdb test\n",
    "for i in range(len(X_test_ptbdb)):\n",
    "    time_series = X_test_ptbdb.to_numpy()[i]\n",
    "\n",
    "    # time_series_fft = np.fft.fft([time_series_x_acc,  time_series_y_acc, time_series_z_acc, time_series_x_giro,  time_series_y_giro, time_series_z_giro])\n",
    "\n",
    "    fe = feature_extractor([time_series], ['series'], W = 100, sample_step=12)\n",
    "    # fe = feature_extractor([*time_series, *time_series_fft], ['dim_0', 'dim_1', 'dim_2', 'dim_3', 'dim_4', 'dim_5', 'dim_0_fft', 'dim_1_fft', 'dim_2_fft', 'dim_3_fft', 'dim_4_fft', 'dim_5_fft'])\n",
    "\n",
    "    results = fe.get_all(no_3D=True)\n",
    "\n",
    "    if i == 0:\n",
    "        features_ptbdb_test = pd.DataFrame.from_dict(results)\n",
    "    else:\n",
    "        features_ptbdb_test = pd.concat([features_mitbih_test, pd.DataFrame.from_dict(results)], ignore_index=True)\n",
    "\n",
    "features_ptbdb_test.to_csv('features_ptbdb_test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functins = ['self.get_max()', 'self.get_min()', 'self.get_std()', 'self.get_ptp()', 'self.get_avg()', 'self.get_median()', 'self.get_mean()', 'self.get_pos()', 'self.get_neg()', 'self.get_iqr()', 'self.get_over_avg()', 'self.get_peaks()', 'self.get_skew()', 'self.get_kurtosis()', 'self.get_energy()', 'self.get_acc()', 'self.get_aria()']\n",
    "names = ['_max', '_min', '_std', '_ptp', '_avg', '_median', '_mean', '_pos', '_neg', '_iqr', '_over_avg', '_peaks', '_skew', '_kurtosis', '_energy', '_acc', '_aria']\n",
    "\n",
    "for i in range(len(functins)):\n",
    "    # print('results' + names[i] + ', labels' + names[i] + ' = ' + functins[i])\n",
    "    # print('*results' + names[i] + ',', end=\" \")\n",
    "    print('*labels' + names[i] + ',', end=\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=None, min_samples_split=2,\n",
    "                          min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='sqrt', max_leaf_nodes=None,\n",
    "                           min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None,\n",
    "                           verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)\n",
    "\n",
    "\n",
    "le = LabelEncoder().fit(y_train_sports)\n",
    "y_sports_train_enc = le.transform(y_train_sports)\n",
    "\n",
    "le = LabelEncoder().fit(y_test_sports)\n",
    "y_sports_test_enc = le.transform(y_test_sports)\n",
    "\n",
    "rf.fit(features_sports_train, y_sports_train_enc)\n",
    "y_pred_sports = rf.predict(features_sports_test)\n",
    "y_pred_proba_sports = rf.predict_proba(features_sports_test)\n",
    "print(y_sports_test_enc)\n",
    "print(y_pred_sports)\n",
    "print(y_pred_proba_sports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False,\n",
    "          tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr',\n",
    "            break_ties=False, random_state=None)\n",
    "\n",
    "svm.fit(features_sports_train, y_sports_train_enc)\n",
    "y_pred_svm_sports = svm.predict(features_sports_test)\n",
    "print(y_sports_test_enc)\n",
    "print(y_pred_svm_sports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree('ECG', ignore_errors=True)\n",
    "shutil.rmtree('RacketSports', ignore_errors=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
